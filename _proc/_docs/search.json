[
  {
    "objectID": "dvc_setup.html",
    "href": "dvc_setup.html",
    "title": "Setting Up DVC",
    "section": "",
    "text": "Data Version Control serves two purposes for our framework.\n\nIt allows us to track large data files that cannot be uploaded to GitHub using the dvc.lock file.\nIt gives structure to our project by properly defining a pipeline with inputs, outputs, parameters, metrics and visualizations.\n\n\nInstallation\nDVC should be installed like any other package by running:\npdm add dvc\n\n\nInitialization\nFrom here, DVC can be initialized by running:\npdm run dvc init\nThis creates a .dvc directory with a few internal files. Add these changes to Git with a commit."
  },
  {
    "objectID": "file_structure.html",
    "href": "file_structure.html",
    "title": "Suggested File Structure",
    "section": "",
    "text": "While the file structure will not make much difference on the correctness of your project, this suggested file structure will encourage code reproducability and explainability. Much of it will be auto generated at this point from the three previous steps, but we’ll add a few things as well as some explanations."
  },
  {
    "objectID": "file_structure.html#pipeline",
    "href": "file_structure.html#pipeline",
    "title": "Suggested File Structure",
    "section": "Pipeline",
    "text": "Pipeline\nWe’ll model our repo structure after the TAI pipeline developed by NDCRANE.\n\nThis pipeline consists of 6 stages of development.\n\nData Collection\nData Preparation\nData Cleaning\nFeature Extraction\nTraining\nInference\n\nWe’ll create one Jupyter Notebook for each stage in our nbs/ directory. It should look like the following (where index.ipynb will convert to our repos’ README.md):\nnbs/\n├── index.ipynb\n├── 01_data_collection.ipynb\n├── 02_data_preparation.ipynb\n├── 03_feature_extraction.ipynb\n├── 04_output_testing.ipynb\n├── 05_training.ipynb\n└── 06_inference.ipynb"
  },
  {
    "objectID": "file_structure.html#data-management",
    "href": "file_structure.html#data-management",
    "title": "Suggested File Structure",
    "section": "Data Management",
    "text": "Data Management\nAs mentioned briefly on the previous page, data will be managed with DVC. We’ll make a directory with in our repo for all data. Here let’s separate data into three categories.\n\nraw/ - import original data from here before cleaning anything\niterim/ - data in the processed of cleaning goes here\ncleaned/ - data ready for training goes here\n\nThus our directory structure for data should look similar to the following:\ndata/\n├── raw/\n| └── raw_data.csv\n├── iterim/\n| └── iterim_data.csv\n├── cleaned/\n└ └── cleaned_data.csv\n\n## Full Structure\nCompiling these in our auto-generated directory from pdm, dvc, and pdm, the repo structure should look similar to the following\n\n```tree\nproject-name/\n├── .gitignore\n├── .pdmtoml\n├── LICENSE\n├── MANIFEST.in\n├── pdm.lock\n├── pyproject.toml\n├── README.md\n├── ssettings.ini\n├── setup.py\n├── styles.css\n├── .github/\n| ├── workflows/\n| | ├── deploy.yaml\n| └ └── test.yaml\n├── .venv/\n| └── ...\n├── data/\n| ├── raw/\n| | └── raw_data.csv\n| ├── iterim/\n| | └── iterim_data.csv\n| ├── cleaned/\n| └ └── cleaned_data.csv\n├── nbs/\n| ├── index.ipynb\n| ├── 01_data_collection.ipynb\n| ├── 02_data_preparation.ipynb\n| ├── 03_feature_extraction.ipynb\n| ├── 04_output_testing.ipynb\n| ├── 05_training.ipynb\n| └── 06_inference.ipynb\n├── model/\n| └── ...\n└── eval/\n| ├── plots/\n└ └── metrics.json"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "frameworks-getting-started",
    "section": "",
    "text": "This written tutorial will show you how to setup and use the tools and best practices developed by the frameworks team. These best practices were inspired and developed from this framework example."
  },
  {
    "objectID": "index.html#example-projects",
    "href": "index.html#example-projects",
    "title": "frameworks-getting-started",
    "section": "Example Projects",
    "text": "Example Projects\nIn addition to this tutorial two other github repositories can be used for refrence. These both include scale implementations of the frameworks toolkit.\n\nNatural Language Processing on FAA Data -> This repo contains a full knoweledge extraction pipeline from data collection to model training and inference using the tools and best practices detailed below.\nComputer Vision analysis on Facial Recongition -> This repo contains analysis on GradCAM output and emphasizes our way of experimentation."
  },
  {
    "objectID": "index.html#recommended-tools",
    "href": "index.html#recommended-tools",
    "title": "frameworks-getting-started",
    "section": "Recommended Tools",
    "text": "Recommended Tools\n\nPDM\n\nPDM, as described, is a modern Python package and dependency manager supporting the latest PEP standards. But it is more than a package manager. It boosts your development workflow in various aspects. The most significant benefit is it installs and manages packages in a similar way to npm that doesn’t need to create a virtualenv at all!\n— PDM Documentation\n\n\n\nnbdev\n\nCreate delightful software with Jupyter Notebooks. Write, test, document, and distribute software packages and technical articles — all in one place, your notebook.\n— nbdev Documentation\n\n\n\nDVC\n\nDVC is built to make ML models shareable and reproducible. It is designed to handle large files, data sets, machine learning models, and metrics as well as code.\n— DVC Documentation"
  },
  {
    "objectID": "dvc_pipeline.html",
    "href": "dvc_pipeline.html",
    "title": "Creating a DVC Pipeline",
    "section": "",
    "text": "Now let’s use the power of DVC. Once we have some of our pipeline built in the nbs/ directory, we can use DVC to automate it using a DVC pipeline."
  },
  {
    "objectID": "dvc_pipeline.html#introducing-the-pipleine",
    "href": "dvc_pipeline.html#introducing-the-pipleine",
    "title": "Creating a DVC Pipeline",
    "section": "Introducing the Pipleine",
    "text": "Introducing the Pipleine\nA DVC pipeline can be initilized from the command line using this documentation, or by following the steps below.\nFirst create a dvc.yaml file in the project root directory. This will serve as the map for our pipeline from which DVC will automate from. Here’s an example three stage pipeline taken from the iterative/example-get-started-experiments repo. We’ll explain what everything means below.\nstages:\n  data_split:\n    cmd: python src/data_split.py\n    deps:\n    - data/pool_data\n    - src/data_split.py\n    params:\n    - base\n    - data_split\n    outs:\n    - data/test_data\n    - data/train_data\n  train:\n    cmd: python src/train.py\n    deps:\n    - data/train_data\n    - src/train.py\n    params:\n    - base\n    - train\n    outs:\n    - models/model.pkl\n  evaluate:\n    cmd: python src/evaluate.py\n    deps:\n    - data/test_data\n    - models/model.pkl\n    - src/evaluate.py\n    params:\n    - base\n    - evaluate\nAs you can see pipelines are defined by stages. Each stage requires a command or cmd: to run the stage, the necessary dependencies or deps: for that stage, and the outputs or outs: from that stage. In addition we can add params: as seen above, and graphs: which will be explained later.\nWhile the pipeline above works perfectly fine, the frameworks team has made a few changes to fit our toolset and best practices. Let’s go over the process for adding a stage now."
  },
  {
    "objectID": "dvc_pipeline.html#how-it-works",
    "href": "dvc_pipeline.html#how-it-works",
    "title": "Creating a DVC Pipeline",
    "section": "How it Works",
    "text": "How it Works\nDVC pipelines are run using:\npdm run dvc repro\nThis ‘reproduces’ the entire pipeline top to bottom, tracking all inputs and outputs in a dvc.lock file. This file looks very similar to the example above, except it also contains the hash and file size for each input and output.\nHeres’ an example of one input entry:\npath: data/pool_data\nmd5: 14d187e749ee5614e105741c719fa185.dir\nsize: 18999874\nThis allows DVC to track exactly what goes in and what comes out with git. Furthermore, on execution of dvc repro DVC checks this .lock file. If the current hashes of all inputs and outputs are the same as all listed in this file, it will not rerun the pipeline. However, if any are missing or different, it will."
  },
  {
    "objectID": "dvc_pipeline.html#the-frameworks-approach",
    "href": "dvc_pipeline.html#the-frameworks-approach",
    "title": "Creating a DVC Pipeline",
    "section": "The Framework’s Approach",
    "text": "The Framework’s Approach\nIn an empty dvc.yml file, add the stages: line at the top and the name of your first stage, like the following:\nstages:\n  data_collection:\nNow let’s add the command line:\n\nRunning Notebooks with Papermill\nIf you are using .py files for each stage, adding python stage.py as the command works perfectly fine. However, if you are using .ipynb files for each stage (accoding to the previous pages) we need to add a few tricks.\nWe’ll use papermill, another iterative.ai tool to run (and parameterize) notebooks. Papermill can be installed with the following:\npdm add papermill\nPapermill works by taking an input notebook, running all cells from top to bottom, then writing all output to a new notebook. Usage is papermill <input_nb> <output_nb>. Since DVC will be running this from our project root, lets add a new directory scripts/ to hold all output notebooks. This can be added to the .gitignore.\nHere’s an example using papermill for our data_collection stage:\nstages:\n  data_collection:\n    cmd: >\n        papermill\n        nbs/00_Collecting_Data.ipynb\n        scripts/00_Collecting_Data.ipynb\n        --cwd nbs/\nThe --cwd nbs/ part executes the notebook from the nbs/ directory. This ensures any imports/exports with relative paths are routed properly.\n\n\nAdding Dependencies\nNow let’s add dependencies for this stage. Obviously we need to add the notebook itself to the deps, but we’ll also add pdm.lock, .pdm.toml, and pyproject.toml. We do this to every stage to track any project dependencies, their versions, and subdependencies required for that notebook. Finally we also add whatever our data source is.\nHere’s an example:\nstages:\n  data_collection:\n    cmd: >\n        papermill \n        nbs/01_data_collection.ipynb \n        scripts/01_data_collection.ipynb \n        --cwd nbs/\n    deps:\n      - nbs/00_Collecting_Data.ipynb\n      - pdm.lock\n      - pyproject.toml\n      - data/raw-data\n      - .pdm.toml\n\n\nAdding Outputs\nFinally we add all outputs from this stage. We’ll add the output script from the papermill command, and whatever data we output. Here’s an example continuing the above:\nstages:\n  data_collection:\n    cmd: >\n        papermill \n        nbs/01_data_collection.ipynb \n        scripts/01_data_collection.ipynb \n        --cwd nbs/\n    deps:\n      - nbs/00_Collecting_Data.ipynb\n      - pdm.lock\n      - pyproject.toml\n      - data/raw-data\n      - .pdm.toml\n    outs:\n      - scripts/00_Collecting_Data.ipynb\n      - data/Concatenated_Orig_data.csv\nThis completes one basic stage. We’ll repeat this process for every stage to fully automate a pipeline\n\n\nAdding Parameters\nIn the introduction above, we discussed the idea of adding parameters to our pipeline. This is an incredibly powerful way to quickly tune hyperparameters of our training script and other stages to compare results. This becomes especially evident once DVC experiments are introduced to the mix. This video demonstrates the motivation here very well.\nLike above, the frameworks team uses a few tricks to parameterize Jupyter Notebooks. This article covers the following in depth. We’ll cover the highlights.\nWe’ll use papermill to paramertize our notebooks. To get started, we’ll add a paremeters cell to 04_training_model.ipynb. To this cell we need to add the parameters tag. Here’s an example:\n\nKFOLD = 1\nTOKENIZER: str = \"bert-base-cased\"\nLEARNING_RATE: float = 5e-5\nBATCH_SIZE: int = 8\nEPOCHS: int = 2\n\nWe can now use these parameters anywhere in our code as variables.\nNext, we’ll create a params.yaml file in our project root. This allows us to set parameters outside of out notebook to be used in our pipeline. With this we can rerun a pipeline using new parameters without editing our notebook.\nHere’s an example of params.yaml:\ntokenizer: bert-base-cased\nlearning_rate: 5e-05\nbatch_size: 8\nepochs: 2\nkfold: 5\nFinally let’s connect the params.yaml to 04_training_model.ipynb using our DVC pipeline. We do this by adding a few parameter or -p tags to our papermill command in dvc.yaml. Here’s an example of a training stage:\ntrain_nn:\n  cmd: >\n    papermill\n    nbs/03a_Training_Model.ipynb\n    scripts/03a_Training_Model.ipynb\n    -p TOKENIZER ${tokenizer}\n    -p LEARNING_RATE ${learning_rate}\n    -p BATCH_SIZE ${batch_size}\n    -p EPOCHS ${epochs}\n    -p KFOLD ${kfold}\n    --cwd nbs/\nFor each parameter we add -p <name of parameter in notebook> ${<name of parameter in params.yaml>}. Be sure to also add params.yaml to the deps: section.\nWhen reproducing the pipeline, papermill will now overwrite the notebook parameters with the corresponding values in params.yaml. Any outputs will reflect these changes."
  },
  {
    "objectID": "pdm_setup.html",
    "href": "pdm_setup.html",
    "title": "Setting Up PDM",
    "section": "",
    "text": "PDM allows us to manage installed packages across versions to ensure when we run our scripts we run them with the correct package and the correct version. To initalize pdm follow the steps below.\n\nInstallation\nPDM should be installed as described in the Installation instructions.\n\n\nInitialization\nOnce PDM is installed and configured, the project should be initialized by running the following command. This will ask a series of questions, where the defaults are usually safe, and produce a file called pyproject.toml.\npdm init\nIMPORTANT: When selecting the python interpreter, select the installation location rather than a virtual environment path. On the CAML cluster this will look something like /opt/anaconda3/bin/python\nIMPORTANT: Ensure PDM sets up a virtual environment for packages to be installed in. This will be the .venv/ directory.\nIf properly initialized the following files will appear in the project root directory:\n\npyproject.toml - contains project metadata and dependencies\npdm.lock - contains the resolved result for dependencies and subdependencies\n.pdm.toml - contains the python interpreter path and package installation location\n.venv/ - contains installed packaged libraries and interpreter\n\n\npyproject.toml\nAdd the name, version, and description fields to the pyproject.toml file. This will ensure the documentation is build correctly.\n\n\n\n.gitignore\nTo prevent conflicts when using git versioning, add the .pdm.toml file and .venv/ directory to your project’s .gitignore\n\n\nAdding Packages\nPackages can be added in the same fasion as pip and conda by running:\npdm add <package_name>\n\n\nRunning Packages\nInstalled packages can be run br prepending pdm run as follows:\npdm run <command_name>\n\n\nJupyter Lab Kernel\nWhen developing in Jupyter Lab, the kernel should point to the PDM environment so installed packages are recognized. This is done as follows:\npython -m ipykernel install --user --name=<virtual_env_name>\nWhere virtual_env_name is the name of the firtual environment pdm points to.\nIf PDM automatically generated a virtual environment (recommended) the name will be {project_name}-{python version}.\nFor example, the venv name for this project is frameworks-getting-started-3.9.\nThis can always be checked by running from the project root:\ncat .pdm.toml\nTo check the project python interpreter. Or by running from the project root:\nsource .venv/bin/activate\nTo view the name of the virtual environment activated.\n\n\nPulling From Github\nWhen pulling from Github it is crucial to ensure local package versions are up to date. To do this run:\npdm sync\nThis will update pdm.lock from pyproject.toml and install packages as needed."
  },
  {
    "objectID": "nbdev_setup.html",
    "href": "nbdev_setup.html",
    "title": "Setting Up nbdev",
    "section": "",
    "text": "nbdev streamlines software development, testing, and documentation all from Jupyter Notebooks using Quarto and Github actions. We’ll use nbdev to increase AI modeling transparency. To initialize nbdev follow the steps below.\n\nInstallation\nWith PDM properly configured we’ll install all packages with it. Let’s start with nbdev.\npdm add nbdev\n\n\nInitialization\n\nThis page was modified from the nbdev End-To-End Walkthrough\n\n\nQuarto\nBefore initializing nbdev, we first need to install quarto. Quarto enables technical publishing which will convert Jupyter Notebooks to beautiful documentation. There are two options for quarto installation.\n\nIf sudo privledges are available you may run:\n\npdm run nbdev_install_quarto\n\nOtherwise, you will have to perform a few tricks:\n\nFirst download the tarball from github.\nwget https://github.com/quarto-dev/quarto-cli/releases/download/v1.2.335/quarto-1.2.335-linux-amd64.tar.gz\nNext extract and build the binaries.\nmkdir quarto\ntar -C ./quarto -xvzf quarto-1.2.335-linux-amd64.tar.gz\nFinally, provide a link to the binaries in the environment, so the interpreter recognizes this installation.\nln -sr ./quarto/quarto-1.2.335/bin/quarto ./.venv/bin/quarto\n(If the environment was configured differently on the previous page, the destination will differ)\nTo ensure properly installation run:\npdm run which quarto\nFrom here you may delete the tarball:\nrm quarto-1.2.335-linux-amd64.tar.gz\n\n\nInit\nWith Quarto installed, now initialize a nbdev repo by entering:\nnbdev_new\nThis will ask for project metadat information that can always be edited in the settings.ini file. Then commit and push the new changes to GitHub.\ngit add . \ngit commit -m 'Initial commit'\ngit push\n\n\nGithub Pages\nGithub pages should be enabled to host the generated documentation. In GitHub click on the “Settings” tab near the top-right of the repo, then “Pages” on the left, then set the “Branch” to gh-pages” and click “Save”.\nIt should look like the following:\n\n\n\nWorkflows\nOn push, nbdev runs two workflows on your project. Here’s a description from the nbdev End-To-End Walkthrough:\nWhat do these workflows do?\n\nCI - The CI (continuous integration) workflow streamlines your developer workflow, particularly with multiple collaborators. Every time you push to GitHub, it ensures that:\n\nYour notebooks and libraries are in sync\nYour notebooks are cleaned of unwanted metadata (which pollute pull requests and git histories and lead to merge conflicts)\nYour notebook tests all pass\n\nDeploy to GitHub Pages - Builds your docs with Quarto and deploys it to GitHub Pages.\n\nIf these run correctly you will see a green check (✅) next to each run. If not you will see a red cross (❌). In this case, click on the cross to see what failed.\nIn some cases you will need to give Workflows “Read and writer permissions. To do this click on”Settings”, go to the “Actions” tab, click “General”, scroll to “Workflow permissions”, then select “Read and write permissions”, finally click “Save”. Here’s a screenshot for refrence:\n\nThe generated docs can be viewed at https://{user}.github.io{repo}\n\n\n\nDeveloping Docs\nWhen working with Jupyter notebooks it is necessary to install nbdev’s hooks to avoid merge conflicts.\npdm run nbdev_install_hooks\nFrom here, documentation can be developed in jupyter notebooks and will be generated on push.\n\n\nindex.ipynb\nnbdev will automatically create an index.ipynb file. This will become the documentation home page and README.md.\n\n\nPushing to GitHub\nBefore commiting changes run pdm run nbdev_prepare. This will perform the following actions:\n\npdm run nbdev_export: Builds the .py modules from Jupyter notebooks\npdm run nbdev_test: Tests your notebooks\npdm run nbdev_clean: Cleans your notebooks to get rid of extreanous output for Github\npdm run nbdev_readme: Updates README.md from your index notebook.\n\nFinally push to GitHub"
  }
]
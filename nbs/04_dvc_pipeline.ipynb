{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a DVC Pipeline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's use the power of DVC.  Once we have some of our pipeline built in the `nbs/` directory, we can use DVC to automate it using a DVC pipeline."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducing the Pipleine\n",
    "\n",
    "A DVC pipeline can be initilized from the command line using this [documentation](\"https://dvc.org/doc/start/data-management/data-pipelines\"), or by following the steps below."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First create a `dvc.yaml` file in the project root directory.  This will serve as the map for our pipeline from which DVC will automate from.  Here's an example three stage pipeline taken from the [iterative/example-get-started-experiments]('https://github.com/iterative/example-get-started-experiments') repo.  We'll explain what everything means below.\n",
    "\n",
    "```yaml\n",
    "stages:\n",
    "  data_split:\n",
    "    cmd: python src/data_split.py\n",
    "    deps:\n",
    "    - data/pool_data\n",
    "    - src/data_split.py\n",
    "    params:\n",
    "    - base\n",
    "    - data_split\n",
    "    outs:\n",
    "    - data/test_data\n",
    "    - data/train_data\n",
    "  train:\n",
    "    cmd: python src/train.py\n",
    "    deps:\n",
    "    - data/train_data\n",
    "    - src/train.py\n",
    "    params:\n",
    "    - base\n",
    "    - train\n",
    "    outs:\n",
    "    - models/model.pkl\n",
    "  evaluate:\n",
    "    cmd: python src/evaluate.py\n",
    "    deps:\n",
    "    - data/test_data\n",
    "    - models/model.pkl\n",
    "    - src/evaluate.py\n",
    "    params:\n",
    "    - base\n",
    "    - evaluate\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see pipelines are defined by stages.  Each stage requires a command or `cmd:` to run the stage, the necessary dependencies or `deps:` for that stage, and the outputs or `outs:` from that stage.  In addition we can add `params:` as seen above, and `graphs:` which will be explained later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the pipeline above works perfectly fine, the frameworks team has made a few changes to fit our toolset and best practices.  Let's go over the process for adding a stage now."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How it Works\n",
    "\n",
    "DVC pipelines are run using:\n",
    "\n",
    "```bash\n",
    "pdm run dvc repro\n",
    "```\n",
    "\n",
    "This 'reproduces' the entire pipeline top to bottom, tracking all inputs and outputs in a `dvc.lock` file.  This file looks very similar to the example above, except it also contains the hash and file size for each input and output.\n",
    "\n",
    "Heres' an example of one input entry:\n",
    "\n",
    "```lock\n",
    "path: data/pool_data\n",
    "md5: 14d187e749ee5614e105741c719fa185.dir\n",
    "size: 18999874\n",
    "```\n",
    "\n",
    "This allows DVC to track exactly what goes in and what comes out with git.  Furthermore, on execution of `dvc repro` DVC checks this `.lock` file.  If the current hashes of all inputs and outputs are the same as all listed in this file, it will not rerun the pipeline.  However, if any are missing or different, it will."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Framework's Approach"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In an empty `dvc.yml` file, add the `stages:` line at the top and the name of your first stage, like the following:\n",
    "\n",
    "```yaml\n",
    "stages:\n",
    "  data_collection:\n",
    "```\n",
    "\n",
    "Now let's add the command line:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running Notebooks with Papermill"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are using `.py` files for each stage, adding `python stage.py` as the command works perfectly fine.  However, if you are using `.ipynb` files for each stage (accoding to the previous pages) we need to add a few tricks.\n",
    "\n",
    "We'll use [papermill](https://papermill.readthedocs.io/en/latest/usage-workflow.html), another [iterative.ai](https://iterative.ai/) tool to run (and parameterize) notebooks.  Papermill can be installed with the following:\n",
    "\n",
    "```bash\n",
    "pdm add papermill\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Papermill works by taking an input notebook, running all cells from top to bottom, then writing all output to a new notebook.  Usage is `papermill <input_nb> <output_nb>`.  Since DVC will be running this from our project root, lets add a new directory `scripts/` to hold all output notebooks.  This can be added to the `.gitignore`.\n",
    "\n",
    "Here's an example using papermill for our `data_collection` stage:\n",
    "\n",
    "```yaml\n",
    "stages:\n",
    "  data_collection:\n",
    "    cmd: >\n",
    "        papermill\n",
    "        nbs/00_Collecting_Data.ipynb\n",
    "        scripts/00_Collecting_Data.ipynb\n",
    "        --cwd nbs/\n",
    "```\n",
    "    \n",
    "The `--cwd nbs/` part executes the notebook from the `nbs/` directory.  This ensures any imports/exports with relative paths are routed properly."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Dependencies"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's add dependencies for this stage.  Obviously we need to add the notebook itself to the deps, but we'll also add `pdm.lock`, `.pdm.toml`, and `pyproject.toml`.  We do this to every stage to track any project dependencies, their versions, and subdependencies required for that notebook.  Finally we also add whatever our data source is. \n",
    "\n",
    "Here's an example:\n",
    "\n",
    "```yaml\n",
    "stages:\n",
    "  data_collection:\n",
    "    cmd: >\n",
    "        papermill \n",
    "        nbs/01_data_collection.ipynb \n",
    "        scripts/01_data_collection.ipynb \n",
    "        --cwd nbs/\n",
    "    deps:\n",
    "      - nbs/00_Collecting_Data.ipynb\n",
    "      - pdm.lock\n",
    "      - pyproject.toml\n",
    "      - data/raw-data\n",
    "      - .pdm.toml\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Outputs\n",
    "\n",
    "Finally we add all outputs from this stage.  We'll add the output script from the `papermill` command, and whatever data we output.  Here's an example continuing the above:\n",
    "\n",
    "```yaml\n",
    "stages:\n",
    "  data_collection:\n",
    "    cmd: >\n",
    "        papermill \n",
    "        nbs/01_data_collection.ipynb \n",
    "        scripts/01_data_collection.ipynb \n",
    "        --cwd nbs/\n",
    "    deps:\n",
    "      - nbs/00_Collecting_Data.ipynb\n",
    "      - pdm.lock\n",
    "      - pyproject.toml\n",
    "      - data/raw-data\n",
    "      - .pdm.toml\n",
    "    outs:\n",
    "      - scripts/00_Collecting_Data.ipynb\n",
    "      - data/Concatenated_Orig_data.csv\n",
    "```\n",
    "\n",
    "This completes one basic stage.  We'll repeat this process for every stage to fully automate a pipeline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Parameters"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the introduction above, we discussed the idea of adding parameters to our pipeline.  This is an incredibly powerful way to quickly tune hyperparameters of our training script and other stages to compare results.  This becomes especially evident once DVC experiments are introduced to the mix.  This [video](https://www.youtube.com/watch?v=iduHPtBncBk) demonstrates the motivation here very well."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like above, the frameworks team uses a few tricks to parameterize Jupyter Notebooks.  This [article](https://iterative.ai/blog/jupyter-notebook-dvc-pipeline/) covers the following in depth.  We'll cover the highlights."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use papermill to paramertize our notebooks.  To get started, we'll add a paremeters cell to `04_training_model.ipynb`.  To this cell we need to add the **parameters tag**.  Here's an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "KFOLD = 1\n",
    "TOKENIZER: str = \"bert-base-cased\"\n",
    "LEARNING_RATE: float = 5e-5\n",
    "BATCH_SIZE: int = 8\n",
    "EPOCHS: int = 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use these parameters anywhere in our code as variables."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll create a `params.yaml` file in our project root.  This allows us to set parameters outside of out notebook to be used in our pipeline.  With this we can rerun a pipeline using new parameters without editing our notebook.\n",
    "\n",
    "Here's an example of `params.yaml`:\n",
    "\n",
    "```yaml\n",
    "tokenizer: bert-base-cased\n",
    "learning_rate: 5e-05\n",
    "batch_size: 8\n",
    "epochs: 2\n",
    "kfold: 5\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally let's connect the `params.yaml` to `04_training_model.ipynb` using our DVC pipeline.  We do this by adding a few parameter or `-p` tags to our `papermill` command in `dvc.yaml`.  Here's an example of a training stage:\n",
    "\n",
    "```yaml\n",
    "train_nn:\n",
    "  cmd: >\n",
    "    papermill\n",
    "    nbs/03a_Training_Model.ipynb\n",
    "    scripts/03a_Training_Model.ipynb\n",
    "    -p TOKENIZER ${tokenizer}\n",
    "    -p LEARNING_RATE ${learning_rate}\n",
    "    -p BATCH_SIZE ${batch_size}\n",
    "    -p EPOCHS ${epochs}\n",
    "    -p KFOLD ${kfold}\n",
    "    --cwd nbs/\n",
    "```\n",
    "\n",
    "For each parameter we add `-p <name of parameter in notebook> ${<name of parameter in params.yaml>}`.  Be sure to also add `params.yaml` to the `deps:` section."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When reproducing the pipeline, papermill will now overwrite the notebook parameters with the corresponding values in `params.yaml`.  Any outputs will reflect these changes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
